# AxiomLabs常见问题与深度对话 (FAQ)

本项目在 Bilibili 和 GitHub 收到了一些非常硬核的技术挑战，现整理如下，以帮助大家更好地理解该架构的底层逻辑。

### Q1: 使用已有的沙盒，公约只在需要的时候给，这样不行吗？形式主义是否会增大模型幻觉？
**A:** PDDL不只是给LLM看的协议，还是事实上的约束。形式化方法是为了逻辑的可验证性。
纯大模型的幻觉来自于它是在概率空间里猜测下一步，这往往只在上下文长度过长或是逻辑过于复杂的情况下才会出现。
我架构是LAMA（后续可能换成其他符号规划器）规划PDDL的路径，这个路径是100%逻辑严密的，LLM只负责在高级层把握最终效果。
现有的沙盒是黑盒，只解决了环境安全的问题，而我的动作具有可解释性，学到的技能会存在本地，已有的沙盒不会。

### Q2: 现在市面上的mcp+skill也类似于这种thinking和coding的代码沙箱，也可以自己定制智能体。我感觉现在agent主要问题是上下文长度(遗忘，听说还有逻辑自循环)，这种架构和像trae，cursor这类智能体底层上有什么区别呢
**A:** MCP是优秀的工具安全调用标准，它倾向于“我相信AI会正确使用这个工具”，而我这个架构则更专注技能的自我创造与演化，思路是“我通过逻辑推导，证明AI必须这样使用工具才能达到目标”。你说的这些智能体trae，cursor这种本质上只是解决人类的某一个问题，而我的架构是在解决一个个人类问题中逐渐学会这些问题该如何解决。就好比一个是标准化的工具库，一个是具备自我演化能力的工具工厂，工具只能执行现有的功能，就算加了新的那也是人加的。而且正如我github的readme所写的，这些由llm直接驱动的模型本质上都是概率模型，让他们提出建议可以，但是如果让他们实际干活，这种概率性就是无法接受的。他们的沙箱是为了更好的执行，而不是约束逻辑。而具备自我演化能力的工具工厂可以自主不断学会新的技能。在我的架构下，pddl的谓词逻辑是确定的，就算llm想删文件，但是如果规则里没写可以报错时可以删文件那他就无法执行。关于你说的记忆问题，我的架构不太依赖上下文，它只是知道自己有什么功能，甚至只知道自己能干哪些领域的事情，然后各个领域的事情让各个领域的llm自己去干，是一种分层的架构，学到的新技能存在本地，不存在遗忘问题。关于逻辑自循环与安全。确实删文件啥的很恐怖，但是这也是目前我的架构避免的方向，而且现在已经有了一些成果。在学会新技能后不仅要通过让其学会这个新技能的任务，也要保证通过以前通过过的任务。

### Q3：这个很像一个bash沙盒。只提供有线的系统命令，对于执行任务产生的二级命令作为记忆存储，并允许动态更新或删除。这个思路很像CC啊，感觉更加简化和轻量级，因为感觉像是函数驱动的。这种感觉更像是用来合成长程ai语料任务的工具
**A:** 二级命令记忆存储也是我最近关注的，感觉可以加进去。函数驱动这块确实是这样，执行函数都遵循统一的抽象接口，只要满足接口契约，理论上可以可以换成其他任何能执行动作的模块。可以用来合成长程ai语料任务我之前确实没有想到。可以在完善后独立出一个转化模型，从而做出长程ai语料任务的数据集，这对后续我自己乃至别人的研发应该都大有裨益。
... (以此类推)