"""CoTæ•°æ®æ‰¹é‡ç”Ÿæˆå·¥å…·

è‡ªåŠ¨åŒ–æ‰¹é‡ç”ŸæˆCoTæ•°æ®ï¼Œæ”¯æŒä»»åŠ¡é˜Ÿåˆ—ã€å¹¶è¡Œå¤„ç†ã€è¿›åº¦ç›‘æ§å’Œæ•°æ®éªŒè¯ã€‚
"""
import os
import sys
import json
import time
import argparse
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from pathlib import Path
import concurrent.futures
import threading
from queue import Queue

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from algorithm.cot_data_generator_with_recorder import create_cot_data_generator_with_recorder
from infrastructure.storage.cot_data_recorder import BatchCoTDataRecorder, create_batch_cot_data_recorder
from infrastructure.llm.deepseek_client import DeepSeekClient
from infrastructure.planner.lama_planner import LAMAPlanner
from config.settings import Settings
from config.data_schema import CoTDataPoint, validate_cot_data


class CoTDataBatchGenerator:
    """CoTæ•°æ®æ‰¹é‡ç”Ÿæˆå™¨"""
    
    def __init__(
        self,
        config: Optional[Dict[str, Any]] = None,
        output_dir: Optional[str] = None,
        max_workers: int = 3,
        use_sandbox: bool = True  # é»˜è®¤å¯ç”¨æ²™ç›’
    ):
        """
        åˆå§‹åŒ–æ‰¹é‡ç”Ÿæˆå™¨
        
        :param config: é…ç½®å­—å…¸
        :param output_dir: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨cot_dataç›®å½•ï¼‰
        :param max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤3ï¼‰
        :param use_sandbox: æ˜¯å¦ä½¿ç”¨æ²™ç›’æ¨¡å¼ï¼ˆé»˜è®¤Trueï¼‰
        """
        self.config = config or {}
        self.max_workers = max_workers
        self.use_sandbox = use_sandbox  # é»˜è®¤å¯ç”¨æ²™ç›’
        
        # è®¾ç½®è¾“å‡ºç›®å½• - é»˜è®¤ä½¿ç”¨cot_dataç›®å½•
        if output_dir is None:
            # é»˜è®¤è¾“å‡ºåˆ°é¡¹ç›®æ ¹ç›®å½•/cot_data/æ—¶é—´æˆ³/
            output_dir = os.path.join(
                project_root,
                "cot_data",
                datetime.now().strftime("%Y%m%d_%H%M%S")
            )
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"ğŸš€ CoTæ•°æ®æ‰¹é‡ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ”’ æ²™ç›’æ¨¡å¼: {'âœ… å¯ç”¨' if use_sandbox else 'âŒ ç¦ç”¨'}")
        print(f"ğŸ‘· å·¥ä½œçº¿ç¨‹: {max_workers}")
        
        # åˆå§‹åŒ–æ‰¹é‡è®°å½•å™¨
        self.batch_recorder = create_batch_cot_data_recorder(self.output_dir)
        
        # ä»»åŠ¡é˜Ÿåˆ—
        self.task_queue = Queue()
        self.results = []
        self.lock = threading.Lock()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_tasks": 0,
            "completed_tasks": 0,
            "successful_tasks": 0,
            "failed_tasks": 0,
            "total_steps": 0,
            "total_errors": 0,
            "start_time": None,
            "end_time": None
        }
    
    def load_tasks_from_file(self, filepath: str) -> List[Dict[str, Any]]:
        """
        ä»æ–‡ä»¶åŠ è½½ä»»åŠ¡åˆ—è¡¨
        
        :param filepath: ä»»åŠ¡æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰
        :return: ä»»åŠ¡åˆ—è¡¨
        """
        with open(filepath, 'r', encoding='utf-8') as f:
            tasks_data = json.load(f)
        
        tasks = []
        if isinstance(tasks_data, list):
            for i, task_item in enumerate(tasks_data):
                if isinstance(task_item, str):
                    tasks.append({
                        "task_id": f"task_{i:04d}",
                        "mission": task_item,
                        "domain": "file-manager-extended"
                    })
                elif isinstance(task_item, dict):
                    task_id = task_item.get("task_id", f"task_{i:04d}")
                    mission = task_item.get("mission", "")
                    domain = task_item.get("domain", "file-manager-extended")
                    tasks.append({
                        "task_id": task_id,
                        "mission": mission,
                        "domain": domain
                    })
        elif isinstance(tasks_data, dict):
            # å•ä¸ªä»»åŠ¡
            tasks.append({
                "task_id": tasks_data.get("task_id", "task_0001"),
                "mission": tasks_data.get("mission", ""),
                "domain": tasks_data.get("domain", "file-manager-extended")
            })
        
        return tasks
    
    def load_default_tasks(self) -> List[Dict[str, Any]]:
        """åŠ è½½é»˜è®¤æµ‹è¯•ä»»åŠ¡"""
        default_tasks = [
            "æ‰«æå½“å‰æ–‡ä»¶å¤¹",
            "åˆ›å»ºä¸€ä¸ªåä¸ºtestçš„æ–‡ä»¶å¤¹",
            "åœ¨testæ–‡ä»¶å¤¹ä¸­åˆ›å»ºREADME.mdæ–‡ä»¶",
            "å°†README.mdæ–‡ä»¶é‡å‘½åä¸ºREADME.txt",
            "å¤åˆ¶README.txtåˆ°backupæ–‡ä»¶å¤¹",
            "åˆ é™¤testæ–‡ä»¶å¤¹",
            "å‹ç¼©backupæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶",
            "è§£å‹archive.zipæ–‡ä»¶åˆ°extractedæ–‡ä»¶å¤¹",
            "è·å–ç®¡ç†å‘˜æƒé™",
            "è¿æ¥ä¸¤ä¸ªæ–‡ä»¶å¤¹",
            "æ‰«æworkspaceæ–‡ä»¶å¤¹å¹¶åˆ›å»ºå¤‡ä»½",
            "å…ˆåˆ›å»ºé¡¹ç›®ç»“æ„ï¼Œç„¶åå¤‡ä»½é‡è¦æ–‡ä»¶",
            "å¦‚æœæ–‡ä»¶å­˜åœ¨åˆ™ç§»åŠ¨å®ƒï¼Œå¦åˆ™åˆ›å»ºæ–°æ–‡ä»¶",
            "é™¤äº†txtæ–‡ä»¶å¤–ï¼Œç§»åŠ¨æ‰€æœ‰æ–‡ä»¶åˆ°archiveæ–‡ä»¶å¤¹"
        ]
        
        tasks = []
        for i, mission in enumerate(default_tasks):
            tasks.append({
                "task_id": f"default_{i:04d}",
                "mission": mission,
                "domain": "file-manager-extended"
            })
        
        return tasks
    
    def add_task(self, task_id: str, mission: str, domain: str = "file-manager-extended"):
        """æ·»åŠ å•ä¸ªä»»åŠ¡åˆ°é˜Ÿåˆ—"""
        self.task_queue.put({
            "task_id": task_id,
            "mission": mission,
            "domain": domain
        })
        with self.lock:
            self.stats["total_tasks"] += 1
    
    def add_tasks(self, tasks: List[Dict[str, Any]]):
        """æ·»åŠ å¤šä¸ªä»»åŠ¡åˆ°é˜Ÿåˆ—"""
        for task in tasks:
            self.task_queue.put(task)
        with self.lock:
            self.stats["total_tasks"] += len(tasks)
    
    def _process_single_task(self, task_info: Dict[str, Any], llm_client, planner) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªä»»åŠ¡
        
        :param task_info: ä»»åŠ¡ä¿¡æ¯
        :param llm_client: LLMå®¢æˆ·ç«¯
        :param planner: è§„åˆ’å™¨
        :return: å¤„ç†ç»“æœ
        """
        task_id = task_info["task_id"]
        mission = task_info["mission"]
        domain = task_info.get("domain", "file-manager-extended")
        
        print(f"  [{task_id}] å¼€å§‹å¤„ç†: {mission}")
        
        try:
            # åˆ›å»ºå¸¦è®°å½•å™¨çš„ç”Ÿæˆå™¨
            generator = create_cot_data_generator_with_recorder(
                llm=llm_client,
                planner=planner,
                config={
                    "domain": domain,
                    "output_dir": os.path.join(self.output_dir, task_id)
                }
            )
            
            # å¼€å§‹ä»»åŠ¡è®°å½•
            batch_recorder = self.batch_recorder.start_task(task_id, mission, domain)
            
            # ç”Ÿæˆæ•°æ®å¹¶è®°å½•
            result = generator.generate_with_recording(mission, save_to_file=False)
            
            # å®Œæˆä»»åŠ¡å¹¶ä¿å­˜æ•°æ®
            filename = f"cot_{task_id}_{int(time.time())}.json"
            filepath = self.batch_recorder.complete_task(task_id, filename)
            
            # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
            task_stats = batch_recorder.get_statistics() if hasattr(batch_recorder, 'get_statistics') else {}
            
            with self.lock:
                self.stats["completed_tasks"] += 1
                self.stats["successful_tasks"] += 1
                self.stats["total_steps"] += task_stats.get("total_steps", 0)
                self.stats["total_errors"] += task_stats.get("total_errors", 0)
            
            print(f"  [{task_id}] âœ… å¤„ç†æˆåŠŸ: {task_stats.get('total_steps', 0)} æ­¥éª¤, "
                  f"{task_stats.get('total_errors', 0)} é”™è¯¯")
            
            return {
                "task_id": task_id,
                "success": True,
                "mission": mission,
                "filepath": filepath,
                "statistics": task_stats,
                "error": None
            }
            
        except Exception as e:
            with self.lock:
                self.stats["completed_tasks"] += 1
                self.stats["failed_tasks"] += 1
            
            print(f"  [{task_id}] âŒ å¤„ç†å¤±è´¥: {e}")
            
            return {
                "task_id": task_id,
                "success": False,
                "mission": mission,
                "filepath": None,
                "statistics": {},
                "error": str(e)
            }
    
    def run(self, use_mock_llm: bool = True, llm_api_key: Optional[str] = None):
        """
        è¿è¡Œæ‰¹é‡ç”Ÿæˆ
        
        :param use_mock_llm: æ˜¯å¦ä½¿ç”¨æ¨¡æ‹ŸLLMï¼ˆç”¨äºæµ‹è¯•ï¼‰
        :param llm_api_key: LLM APIå¯†é’¥ï¼ˆå¦‚æœä¸ä½¿ç”¨æ¨¡æ‹ŸLLMï¼‰
        """
        print("=" * 60)
        print("ğŸš€ CoTæ•°æ®æ‰¹é‡ç”Ÿæˆå™¨å¯åŠ¨")
        print("=" * 60)
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"æœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers}")
        print(f"æ€»ä»»åŠ¡æ•°: {self.stats['total_tasks']}")
        print("-" * 60)
        
        self.stats["start_time"] = datetime.now()
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        if use_mock_llm:
            print("ä½¿ç”¨æ¨¡æ‹ŸLLMï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
            
            class MockLLM:
                def chat(self, messages, temperature=0.1):
                    # æ ¹æ®æ¶ˆæ¯å†…å®¹è¿”å›ä¸åŒçš„æ¨¡æ‹Ÿå“åº”
                    content = messages[-1]["content"] if messages else ""
                    
                    if "æ‰«æ" in content or "scan" in content.lower():
                        return "(scan workspace)"
                    elif "åˆ›å»º" in content or "create" in content.lower():
                        return "(create_folder test)\n(create_file README.md)"
                    elif "ç§»åŠ¨" in content or "move" in content.lower():
                        return "(move file1 workspace backup)"
                    elif "é‡å‘½å" in content or "rename" in content.lower():
                        return "(rename file1 file2)"
                    elif "å¤åˆ¶" in content or "copy" in content.lower():
                        return "(copy file1 workspace backup)"
                    elif "åˆ é™¤" in content or "delete" in content.lower():
                        return "(remove file1)"
                    elif "å‹ç¼©" in content or "compress" in content.lower():
                        return "(compress file1 archive.zip)"
                    elif "è§£å‹" in content or "uncompress" in content.lower():
                        return "(uncompress archive.zip extracted)"
                    else:
                        return "(scan workspace)\n(create_folder backup)\n(move file1 workspace backup)"
            
            llm_client = MockLLM()
            planner = None
            
        else:
            print("ä½¿ç”¨çœŸå®LLMï¼ˆç”Ÿäº§æ¨¡å¼ï¼‰")
            if not llm_api_key:
                raise ValueError("ç”Ÿäº§æ¨¡å¼éœ€è¦æä¾›LLM APIå¯†é’¥")
            
            # åŠ è½½é…ç½®
            settings = Settings.load_from_env()
            settings.llm_api_key = llm_api_key
            
            # åˆ›å»ºçœŸå®LLMå®¢æˆ·ç«¯
            llm_client = DeepSeekClient(
                api_key=settings.llm_api_key,
                base_url=settings.llm_base_url,
                model=settings.llm_model
            )
            
            # åˆ›å»ºè§„åˆ’å™¨
            planner = LAMAPlanner(config=settings)
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†ä»»åŠ¡
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            while not self.task_queue.empty():
                task_info = self.task_queue.get()
                future = executor.submit(self._process_single_task, task_info, llm_client, planner)
                futures.append(future)
            
            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(futures):
                try:
                    result = future.result()
                    self.results.append(result)
                except Exception as e:
                    print(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
        
        # æ›´æ–°ç»“æŸæ—¶é—´
        self.stats["end_time"] = datetime.now()
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_report()
    
    def _generate_report(self):
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ‰¹é‡å¤„ç†æŠ¥å‘Š")
        print("=" * 60)
        
        # è®¡ç®—è€—æ—¶
        if self.stats["start_time"] and self.stats["end_time"]:
            duration = self.stats["end_time"] - self.stats["start_time"]
            print(f"æ€»è€—æ—¶: {duration}")
        
        print(f"æ€»ä»»åŠ¡æ•°: {self.stats['total_tasks']}")
        print(f"å·²å®Œæˆä»»åŠ¡: {self.stats['completed_tasks']}")
        print(f"æˆåŠŸä»»åŠ¡: {self.stats['successful_tasks']}")
        print(f"å¤±è´¥ä»»åŠ¡: {self.stats['failed_tasks']}")
        
        if self.stats['completed_tasks'] > 0:
            success_rate = (self.stats['successful_tasks'] / self.stats['completed_tasks']) * 100
            print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        
        print(f"æ€»æ­¥éª¤æ•°: {self.stats['total_steps']}")
        print(f"æ€»é”™è¯¯æ•°: {self.stats['total_errors']}")
        
        if self.stats['total_steps'] > 0:
            error_rate = (self.stats['total_errors'] / (self.stats['total_steps'] + self.stats['total_errors'])) * 100
            print(f"é”™è¯¯ç‡: {error_rate:.1f}%")
        
        # æ‰¹é‡è®°å½•å™¨æ‘˜è¦
        batch_summary = self.batch_recorder.get_summary()
        print(f"\nğŸ“¦ æ•°æ®è®°å½•æ‘˜è¦:")
        print(f"  æ€»æ•°æ®ç‚¹: {batch_summary.get('total_tasks', 0)}")
        print(f"  æ€»æ­¥éª¤: {batch_summary.get('total_steps', 0)}")
        print(f"  æ€»é”™è¯¯: {batch_summary.get('total_errors', 0)}")
        print(f"  æˆåŠŸç‡: {batch_summary.get('success_rate', 0):.1f}%")
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = os.path.join(self.output_dir, "batch_report.json")
        
        # å‡†å¤‡æŠ¥å‘Šæ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰datetimeå¯¹è±¡éƒ½è¢«è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        stats_copy = self.stats.copy()
        if stats_copy.get("start_time") and isinstance(stats_copy["start_time"], datetime):
            stats_copy["start_time"] = stats_copy["start_time"].isoformat()
        if stats_copy.get("end_time") and isinstance(stats_copy["end_time"], datetime):
            stats_copy["end_time"] = stats_copy["end_time"].isoformat()
        
        report_data = {
            "stats": stats_copy,
            "batch_summary": batch_summary,
            "results_summary": [
                {
                    "task_id": r["task_id"],
                    "success": r["success"],
                    "mission": r["mission"],
                    "filepath": r["filepath"]
                }
                for r in self.results
            ],
            "output_dir": self.output_dir,
            "generated_at": datetime.now().isoformat()
        }
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # å¯¼å‡ºæ‰€æœ‰è®­ç»ƒæ•°æ®
        try:
            exported = self.batch_recorder.export_all_training_data()
            print(f"\nğŸ¯ è®­ç»ƒæ•°æ®å·²å¯¼å‡º:")
            print(f"  BrainLLMæ•°æ®: {len(exported.get('brain_files', []))} ä¸ªæ–‡ä»¶")
            print(f"  NervesLLMæ•°æ®: {len(exported.get('nerves_files', []))} ä¸ªæ–‡ä»¶")
            print(f"  AnalysisLLMæ•°æ®: {len(exported.get('analysis_files', []))} ä¸ªæ–‡ä»¶")
        except Exception as e:
            print(f"\nâš ï¸  è®­ç»ƒæ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰¹é‡ç”Ÿæˆå®Œæˆ!")
        print("=" * 60)
    
    def get_results(self) -> List[Dict[str, Any]]:
        """è·å–å¤„ç†ç»“æœ"""
        return self.results
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats


def main():
    """å‘½ä»¤è¡Œä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="CoTæ•°æ®æ‰¹é‡ç”Ÿæˆå·¥å…·")
    parser.add_argument("--tasks-file", type=str, help="ä»»åŠ¡æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰")
    parser.add_argument("--output-dir", type=str, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--workers", type=int, default=3, help="å·¥ä½œçº¿ç¨‹æ•°")
    parser.add_argument("--use-real-llm", action="store_true", help="ä½¿ç”¨çœŸå®LLMï¼ˆéœ€è¦APIå¯†é’¥ï¼‰")
    parser.add_argument("--api-key", type=str, help="LLM APIå¯†é’¥")
    parser.add_argument("--default-tasks", action="store_true", help="ä½¿ç”¨é»˜è®¤æµ‹è¯•ä»»åŠ¡")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ‰¹é‡ç”Ÿæˆå™¨
    generator = CoTDataBatchGenerator(
        output_dir=args.output_dir,
        max_workers=args.workers
    )
    
    # åŠ è½½ä»»åŠ¡
    if args.tasks_file:
        print(f"ä»æ–‡ä»¶åŠ è½½ä»»åŠ¡: {args.tasks_file}")
        tasks = generator.load_tasks_from_file(args.tasks_file)
        generator.add_tasks(tasks)
    elif args.default_tasks:
        print("ä½¿ç”¨é»˜è®¤æµ‹è¯•ä»»åŠ¡")
        tasks = generator.load_default_tasks()
        generator.add_tasks(tasks)
    else:
        print("é”™è¯¯: å¿…é¡»æä¾›ä»»åŠ¡æºï¼ˆ--tasks-file æˆ– --default-tasksï¼‰")
        parser.print_help()
        return
    
    # è¿è¡Œæ‰¹é‡ç”Ÿæˆ
    use_mock_llm = not args.use_real_llm
    llm_api_key = args.api_key
    
    try:
        generator.run(use_mock_llm=use_mock_llm, llm_api_key=llm_api_key)
    except Exception as e:
        print(f"æ‰¹é‡ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def quick_test():
    """å¿«é€Ÿæµ‹è¯•å‡½æ•°"""
    print("è¿è¡Œå¿«é€Ÿæµ‹è¯•...")
    
    # åˆ›å»ºæ‰¹é‡ç”Ÿæˆå™¨
    generator = CoTDataBatchGenerator(
        output_dir=os.path.join(project_root, "workspace", "test_batch"),
        max_workers=2
    )
    
    # æ·»åŠ å‡ ä¸ªæµ‹è¯•ä»»åŠ¡
    test_tasks = [
        {"task_id": "test_001", "mission": "æ‰«æworkspaceæ–‡ä»¶å¤¹", "domain": "file-manager-extended"},
        {"task_id": "test_002", "mission": "åˆ›å»ºtestæ–‡ä»¶å¤¹", "domain": "file-manager-extended"},
        {"task_id": "test_003", "mission": "ç§»åŠ¨æ–‡ä»¶åˆ°backup", "domain": "file-manager-extended"},
    ]
    
    generator.add_tasks(test_tasks)
    
    # è¿è¡Œæµ‹è¯•ï¼ˆä½¿ç”¨æ¨¡æ‹ŸLLMï¼‰
    print("ä½¿ç”¨æ¨¡æ‹ŸLLMè¿è¡Œæµ‹è¯•...")
    generator.run(use_mock_llm=True)
    
    # æ˜¾ç¤ºç»“æœ
    results = generator.get_results()
    print(f"\næµ‹è¯•å®Œæˆï¼Œå¤„ç†äº† {len(results)} ä¸ªä»»åŠ¡")
    
    for result in results:
        status = "âœ… æˆåŠŸ" if result["success"] else "âŒ å¤±è´¥"
        print(f"  {result['task_id']}: {status} - {result['mission']}")
    
    return generator


if __name__ == "__main__":
    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œè¿è¡Œå¿«é€Ÿæµ‹è¯•
    if len(sys.argv) == 1:
        print("æœªæä¾›å‘½ä»¤è¡Œå‚æ•°ï¼Œè¿è¡Œå¿«é€Ÿæµ‹è¯•...")
        quick_test()
    else:
        main()